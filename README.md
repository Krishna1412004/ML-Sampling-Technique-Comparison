# **Machine Learning Model Evaluation with Different Sampling Techniques**

## **Project Overview**

This project evaluates the performance of five machine learning models using five different sampling techniques. The aim is to identify the best model-sampling combination based on key evaluation metrics.

### **Models Used**
1. **Logistic Regression**
2. **Random Forest Classifier**
3. **Support Vector Machine (SVM)**
4. **Decision Tree Classifier**
5. **K-Nearest Neighbors (KNN)**

### **Sampling Techniques**
1. **Random Sampling**
2. **Systematic Sampling**
3. **Stratified Sampling**
4. **Cluster Sampling**
5. **Bootstrap Sampling**

### **Evaluation Metrics**
- **Accuracy**
- **Precision**
- **Recall**
- **F1-Score**
- **ROC-AUC**

---

## **Results**

The project identifies the sampling technique that achieves the highest accuracy for each model. Key findings include:
- **Stratified Sampling** tends to perform well with imbalanced datasets.
- **Random Sampling** often complements flexible models like Random Forest.
- **Bootstrap Sampling** can be effective for ensemble models by providing diverse training data.

For detailed results, refer to the output generated by the script or the [Discussion](discussion.md) file.

---

## **Discussion**

A thorough discussion of the project's results and insights is available in the [Discussion](discussion.md) file. It explains the effectiveness of various sampling techniques and their impact on model performance.

---

## **Contributing**

Contributions are welcome! Fork the repository, open issues, and create pull requests to enhance this project.

---

## **License**

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.

---

## **Acknowledgments**

- **Scikit-learn** for providing the machine learning models and utilities.
- Online resources and tutorials for guiding the implementation of sampling techniques and model evaluation.
